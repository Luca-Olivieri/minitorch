# Log Book

- 2026-01-15: 
    I started implementing the backpropagation graph logic. Leaf nodes and operations are extensions of a Node class and implement logic to backpropagate starting from the topmost node.
- 2026-01-15:
    I recognized that I do know how to feed this logic to a tensors. Hence, I started implementing tensors.
- 2026-01-15:
    I realised I do not have a testing framework. I understood that Catch2 and Google Test are the standard way to go, but I do not want to integrate external libraries for something so simple, so I will make my own testing functions.
- 2026-01-31: 
    I implemented reshape(), transpose() and slice() tensor methods, which operator solely on the strides without touching the array memory.
    However, to iterate through restrided data in a clean way, I need to use some custom iterators, which have some overhead. To speed this up, ChatGPT told me to create a stride-counter iterator, to collapse contiguous dimensions, and to create logic to make restrided data contiguous (creating new data). I will do this in a second moment, I will keep using this naive but correct implementation and see how to optimize it later on.
- 2026-01-31: implemented the dice() tensor methods, reindexing operators are easier to implement than expected.
- 2026-02-02: I realized that having a plain tensor<-->backward_ops is too much spaghetti. Instead, I will define a TensorImpl class implementing the data structure, the indexing and the basic untracked tensorial operations. Then, I will implement the Tensor class holding a TensorImpl for the value and a shared_ptr for the Grad, as well as other autograd metadata. Tensor will act as interface for TensorImpl and the operations will implement the computation graph logic with the help of the BackwardOp in "autograd.h". "autograd.h" will include "tensors.h", while "tensors.h" will not include the "autograd.h", but will have a simple class forward declaration to the abstract class BackwardOp. "tensors.cpp" will include "autograd.h" to reimplement the Tensor.backward () as a wrapper of some BackwardOp.backward() (with solely the class forward declaration tensors have no access to the BackwardOp methods). This kind-of cyclic dependency is required because, to access higher-order derivatives, grads should create other grads, but that is solely doable accessing tensor operations, but tensors must hold information of the grads to act as unique interface (as in PyTorch). Otherwise, I could implement Tensor without grad information, and implement the autograd logic above tensors, but Tensors interface would not be as complete.
- 2026-02-08: the higher-order derivative work properly, but inplace operations break the graph, how should I treat them for differentiability? Is there even a way to make them differentiable? Also, the graph has some circular shared_ptr dependencies that makes it impossible for some nodes to be dropped. Investigate, and assess if weak_ptr should be used in some places instead.
- 2026-02-09: I discovered that TensorStorage.reshape(...) cannot mathematically work on non-contiguous tensors (without first making the contiguous), hence the method should create a contiguous copy in that case. I am, for now, deleting the functionality of views because they are making me mad.

